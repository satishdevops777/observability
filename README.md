# observability
 - Observability is the capability of a system to explain its internal state using telemetry data like logs, metrics, and traces.

ğŸ‘‰ If your system breaks at 3 AM, observability tells you what broke, where, why, and what to do next.

## Why Observability Exists
### Traditional monitoring answers:
- â“ Is the system up?
- â“ Is CPU high?
### Observability answers:
- âœ… Which service is slow?
- âœ… Which user request failed?
- âœ… What change caused the outage?
- âœ… Is this infra, app, or dependency related?

## The Three Pillars of Observability ğŸ§±
### 1ï¸âƒ£ Metrics (What is happening?)
- Numeric values over time
- Fast, cheap, good for alerts
### Examples
- CPU usage
- Memory usage
- Request rate (RPS)
- Error rate
- Latency (P95, P99)
### Tools
- Prometheus
- Grafana
- CloudWatch
- SignalFx

### 2ï¸âƒ£ Logs (Why did it happen?)

<img width="868" height="511" alt="image" src="https://github.com/user-attachments/assets/d599ac15-f8c3-4b6e-98c5-a83a794c8c7e" />

- Detailed, contextual records
- Best for debugging
```ERROR payment-service timeout while calling bank-api```
### Tools
- Splunk
- ELK / OpenSearch
- CloudWatch Logs

### 3ï¸âƒ£ Traces (Where did it happen?)
- Follow a request across services
- Shows latency & bottlenecks
- User â†’ API Gateway â†’ Auth â†’ Order â†’ Payment â†’ DB

### Tools
- Jaeger
- Zipkin
- OpenTelemetry
- X-Ray

```â€œObservability allows us to understand system behaviour and diagnose unknown failures using metrics, logs, and traces, reducing MTTR and improving reliability.â€```

## Splunk and SignalFx
- Splunk = logs (deep details, forensics)
- Signal = metrics + monitoring (real-time health, alerts)
### Together, they answer:
- Is something wrong? â†’ SignalFx
- Where is it wrong? â†’ SignalFx
- Why did it happen? â†’ Splunk
 

### Components Youâ€™re Using
### Splunk
- Centralized log storage & search
- Root cause analysis
- Incident forensics
### SignalFx
- Metrics, dashboards, alerts
- Real-time monitoring
- APM & infrastructure visibility (Now part of Splunk Observability Cloud)

## 1ï¸âƒ£ Monitoring with SignalFx (Metrics-Driven)
### What you monitor in SignalFx
### ğŸ”¹ Infrastructure Metrics
- CPU, memory, disk, network
- Node health
- Load average
### ğŸ”¹ Kubernetes Metrics
- Pod restarts
- Pod CPU/memory throttling
- Node pressure
- Deployment replica health
###ğŸ”¹Application Metrics (Golden Signals â­)
- Latency (P95 / P99)
- Traffic (RPS)
- Errors (4xx / 5xx)
- Saturation

### How data gets into SignalFx
- SignalFx Smart Agent / OpenTelemetry
- K8s metrics-server
- Cloud integrations (AWS, Azure, GCP)
- App instrumentation (Java, Node, Python, etc.)

Example Alert in SignalFx ğŸš¨
 
ğŸ‘‰ This tells you something is wrong
But not yet why.
2ï¸âƒ£ Logging & Deep Debugging with Splunk
What goes into Splunk
â€¢	Application logs
â€¢	Kubernetes pod logs
â€¢	Node/system logs
â€¢	API gateway logs
â€¢	Error & exception traces


Example Logs in Splunk ğŸ”

 
Now you know:
â€¢	Which service
â€¢	Which dependency
â€¢	Exact failure reason

3ï¸âƒ£ How Monitoring + Observability Work Together

ğŸš¨ Incident Flow (Real World)
Step 1: Alert fires in SignalFx
High latency in payment-service

### Step 2: Open SignalFx dashboard
You see:
- CPU normal âŒ
- Memory normal âŒ
- Latency high âœ…
- Error rate rising âœ…
ğŸ‘‰ Problem is application or dependency, not infra

### Step 3: Pivot to Splunk (Logs)
- service=payment-service environment=prod ERROR
- DB connection pool exhausted

### Step 4: Root Cause
- Traffic spike
- DB max connections reached
- App retries increased latency

### Step 5: Fix
- Scale DB
- Increase pool
- Add rate limit / caching
- MTTR drops drastically

## 4ï¸âƒ£ Correlation (This Is Observability ğŸ”¥)

| SignalFx     | Splunk          |
| ------------ | --------------- |
| Alert        | Error logs      |
| Metric spike | Stack trace     |
| Dashboard    | Request context |
| SLO breach   | Root cause      |

ğŸ’¡ Same service name, host, pod, request ID

## 5ï¸âƒ£ SLOs with SignalFx + Splunk
### SignalFx
- Tracks SLI metrics (latency, error rate)
- Calculates SLO compliance
- Alerts on burn rate

### Splunk
- Explains why SLO was violated
- Provides evidence for RCA

## 6ï¸âƒ£ How Youâ€™d Explain This in an Interview ğŸ¤
- â€œWe use SignalFx for real-time monitoring and alerting based on metrics like latency, error rate, and saturation. When an alert fires, we correlate it with logs stored in Splunk to perform root cause analysis. This combination allows us to detect issues early, reduce MTTR, and handle both known and unknown failure scenarios effectively.â€


## 7ï¸âƒ£ Summary (One Screen ğŸ§ )

### âœ… SignalFx
- Monitoring
- Dashboards
- Alerts
- SLOs

### âœ… Splunk
- Logs
- Debugging
- RCA
- Audit & forensics

### âœ… Together
- = True Observability


